%-------------------------------------------------------------------------%
%  Hyper Learning Binary Dragonfly Algorithm source code demo version     %
%-------------------------------------------------------------------------%


%---Inputs-----------------------------------------------------------------
% feat:   features
% label:  labelling
% N:      Number of dragonflies
% T:      Maximum number of iterations
% pl:     Probability of personal learning
% gl:     Probability of global learning
% kfold:  Number of K-fold cross-validation
% k:      Number of k in KNN
%---Outputs----------------------------------------------------------------
% sFeat:  Selected features
% Sf:     Selected feature index
% Nf:     Number of selected features
% curve:  Convergence curve
%--------------------------------------------------------------------------

%Test
clc; clear; close;

datasets = { '1-Arrhythmia.mat', '2-colon.mat', '3-dermatology.mat', '4-glass.mat', '5-hepatitis.mat', ...
            '6-horse-colic.mat', '7-ilpd.mat', '8-ionosphere.mat', '9-leukemia.mat', '10-libras-movement.mat', ...
            '11-lsvt.mat', '12-lung_discrete.mat', '13-lympho.mat', '14-musk1.mat', '15-primary-tumor.mat', ...
            '16-SCADI.mat', '17-seeds.mat', '18-soybean.mat', '19-spect-heart.mat', '20-TOX-171.mat', '21-zoo.mat'};

% Define the path to the folder containing the datasets
folder_path = 'datasets/';

% Store statistics
stats = cell(length(datasets), 2);

% Set parameters
kfold = 10;k = 5;N = 10;T = 100;
%%%%%%%%%%%%%%%%%%%%%%Adjustable parameters%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
parties = 8;        %Number of political parties
lambda = 1.0;       %Max limit of party switching rate

%fEvals = 30000;     %Number of function evaluations
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

areas = parties;
members = parties;                
populationSize=parties * members; % Number of search agents
Max_iteration = 100; %round(fEvals / (parties * areas + areas));
pl = 0.4; % Personal learning rate
gl = 0.7; % Global learning rate
runs = 10;

%change plr & glr, combination in HLBDA pg 7

execution_times = zeros(1, length(datasets));

for i = 1:length(datasets)
    
    % Measure execution time
    tic;
    
    O.k = k;O.kfold = kfold;O.N = N;O.T = T;O.pp = pl;O.pg = gl;
    
    % Load data
    dataset_name = strcat(folder_path, datasets{i});
    load(dataset_name);
    
    % Determine variable names for features and labels
    if exist('feat', 'var') && exist('label', 'var')
        X = feat;
        Y = label;
    elseif exist('X', 'var') && exist('Y', 'var')
        % Assuming X is features and Y is labels
    elseif exist('Hepatitis', 'var') && exist('Class', 'var')
        X = Hepatitis;
        Y = Class;
    elseif exist('X', 'var') && exist('y', 'var')
        % Assuming X is features and y is labels
        Y = y;
    else
        error('Could not determine variable names for features and labels.');
    end
    
    % Xi are enteries while Xj are features
    % Yi are enteries while Yj are outcomes / labels

    % Split data into train & validate using cross-validation
    CV = cvpartition(Y, 'KFold', kfold, 'Stratify', true);
    O.Model = CV; 
    
%     % Perform feature selection
     [sFeat,Sf,Nf,curve] = jHLBDA(X, Y, O); 
%         
%     % Accuracy 
     Acc = jKNN(sFeat,Y,CV,O); 

%     % Calling algorithm
%     B_Best_score_T = zeros(1,runs);
%     HLB_Best_score_T = zeros(1,runs);
%     
%     for run=1:runs
%         rng('shuffle');
%         %[B_Best_score_0,B_Best_pos,B_PO_cg_curve] =             BPO(populationSize,areas,parties,lambda,Max_iteration,pl,gl,Y,O);
%         %B_Best_score_T(1,run) = B_Best_score_0;
%         
%       [HLB_Best_score_0,HLB_Best_pos,HLB_PO_cg_curve] =       HLBPO(populationSize,parties,areas,lambda,Max_iteration,pl,gl,X,Y,O);
%        HLB_Best_score_T(1,run) = HLB_Best_score_0;
%         
%         %Best_score_0
%     end

    execution_time = toc; % Stop measuring time
    
    execution_times(i) = execution_time;

    %Acc = 0; 
    % Store statistics
    stats{i, 1} = dataset_name;
    stats{i, 2} = Acc;

    % Specify the folder path
    folder_path1 = 'img/';

    % Check if the folder exists, if not, create it
    if ~exist(folder_path1, 'dir')
        mkdir(folder_path1);
    end
    
    % Concatenate folder path, dataset name, and the desired suffix
    filename = strcat(folder_path1, datasets{i}, '.png');

    % Plot the convergence curve and save with custom filename
    % colors = {
    %     'r', [1, 0, 0];         % Red
    %     'g', [0, 1, 0];         % Green
    %     'b', [0, 0, 1];         % Blue
    %     'c', [0, 1, 1];         % Cyan
    %     'm', [1, 0, 1];         % Magenta
    %     'y', [1, 1, 0];         % Yellow
    %     'k', [0, 0, 0];         % Black
    %     'w', [1, 1, 1];         % White
    %     'lightblue', [0.68, 0.85, 0.9];    % Light Blue
    %     'lightgreen', [0.56, 0.93, 0.56];  % Light Green
    %     'orange', [1, 0.65, 0];            % Orange
    %     'purple', [0.63, 0.13, 0.94];      % Purple
    %     'pink', [1, 0.75, 0.8];            % Pink
    %     'brown', [0.64, 0.16, 0.16];       % Brown
    % };
    
    % Define colors for each curve
    colors = {'r', 'g', 'b'};
    
    %B_PO_cg_curve = curve - 0.05;
    %HLB_PO_cg_curve = curve - 0.1;

    % Plot the convergence curves and save as a single image
    plotConvergenceCurve( ...
        {curve, B_PO_cg_curve, HLB_PO_cg_curve}, ...
        filename, ...
        {'HLBDA', 'BPO', 'HLBPO'}, ...
        colors);



    %Finding statistics
    %B_Best_score_Best = min(B_Best_score_T);
    %HLB_Best_score_Best = min(HLB_Best_score_T);
    %Best_score_Worst = max(Best_score_T);
    %Best_score_Median = median(Best_score_T,2);
    %Best_Score_Mean = mean(Best_score_T,2);
    %Best_Score_std = std(Best_score_T);


    %acc table & feature selection ratio & classification acc & box plot


    %Printing results
    disp([stats{i, 1}, ', ', num2str(stats{i, 2})]);
    %display(['Fn = ', num2str(fn)]);
    %display(['Best, Worst, Median, Mean, and Std. are as: ', num2str(Best_score_Best),'  ', num2str(Best_score_Worst),'  ', num2str(Best_score_Median),'  ', num2str(Best_Score_Mean),'  ', num2str(Best_Score_std)]);

end



% Display statistics
disp('Dataset Name, Accuracy');
for i = 1:length(datasets)
    disp([stats{i, 1}, ', ', num2str(stats{i, 2})]);
end


% Display overall execution times
total_execution_time = sum(execution_times);
disp(['Total execution time for all functions: ', num2str(total_execution_time), ' seconds']);

function plotConvergenceCurve(curves, filename, labels, colors)
    % Create a figure without displaying it
    fig = figure('Visible', 'off');

    % Plot all convergence curves with labels and colors
    hold on;
    for i = 1:length(curves)
        plot(1:length(curves{i}), curves{i}, 'LineWidth', 2, 'DisplayName', labels{i}, 'Color', colors{i});
    end
    hold off;

    % Customize the plot
    title('Convergence Curves');
    xlabel('Iteration');
    ylabel('Fitness');
    grid on;
    legend('show');
    
    % Delete the existing file if it exists
    if nargin > 1 && exist(filename, 'file') == 2
        delete(filename);
    end
    
    % Save the plot with custom filename
    if nargin > 1
        saveas(fig, filename);
    end
    
    % Close the figure
    close(fig);
end

